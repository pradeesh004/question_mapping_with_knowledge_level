# Critical Thinking Question Generator & Evaluation System

A RAG (Retrieval-Augmented Generation) based system that generates critical thinking questions from PDF documents and evaluates them using multiple language models with Bloom's Taxonomy framework.

## üéØ Overview

This system analyzes PDF documents to generate critical thinking questions and evaluates their quality using three different language models. It implements a comprehensive evaluation framework based on Bloom's Taxonomy knowledge levels (K1-K6) to assess the cognitive complexity of generated questions.

## ‚ú® Features

- **PDF Document Processing**: Load and chunk PDF documents for analysis
- **Multi-Model Question Generation**: Generate questions using 3 different LLMs
- **Critical Thinking Framework**: Questions based on WHO, WHAT, WHERE, WHEN, WHY, HOW dimensions
- **Bloom's Taxonomy Evaluation**: Classify questions by cognitive complexity (K1-K6)
- **Cross-Model Evaluation**: Each model evaluates questions from all other models
- **Robust Text Processing**: Advanced regex patterns for question extraction

## üõ†Ô∏è Dependencies

```bash
pip install langchain-community
pip install langchain
pip install langchain-ollama
pip install faiss-cpu
pip install langchain-groq
pip install pypdf
```

## üìã Requirements

### API Keys
- **Groq API Key**: Required for ChatGroq models
- **Ollama**: Must be running locally with `nomic-embed-text:latest` model

### Models Used
- **DeepSeek R1 Distill Llama 70B**: Primary question generator
- **Google Gemma 2 9B**: Secondary question generator  
- **OpenAI GPT OSS 120B**: Tertiary question generator

### File Requirements
- PDF document at specified path: `Q:\projects\ctp\critical_thinking_user answer evaluation\attention-is-all-you-need-Paper.pdf`

## üöÄ Installation & Setup

1. **Clone/Download** the script to your local environment

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Setup Ollama**:
   ```bash
   ollama pull nomic-embed-text:latest
   ollama serve
   ```

4. **Configure API Key**:
   - Replace the Groq API key in the `llm()` function with your own key
   - Or set it as an environment variable

5. **Update File Path**:
   - Modify the PDF path in the `rag()` function to point to your document

## üíª Usage

1. **Run the Script**:
   ```bash
   python main.py
   ```

2. **Enter Topic**: When prompted, enter the topic you want to generate questions about

3. **View Results**: The system will output:
   - Questions generated by each model
   - Evaluations from each model using Bloom's Taxonomy

## üìñ System Architecture

### 1. RAG Pipeline (`rag()`)
- Loads PDF using PyPDFLoader
- Splits text into chunks (1000 chars, 500 overlap)
- Creates FAISS vector store with Ollama embeddings
- Returns retriever for context-aware question generation

### 2. Question Generation (`question_promtp()`)
- Uses critical thinking dimensions (WHO, WHAT, WHERE, WHEN, WHY, HOW)
- Generates exactly 3 questions per model
- Focuses on analysis, evaluation, synthesis, and application
- Ensures questions are answerable from provided content

### 3. Question Evaluation (`evalutaion_prompt()`)
- Maps questions to Bloom's Taxonomy levels (K1-K6)
- Analyzes cognitive demands and primary verbs
- Provides structured evaluation format

### 4. Text Processing
- `extract_questions_only()`: Extracts exactly 3 questions using multiple regex patterns
- `clean_evaluation_response()`: Removes thinking tags and formats evaluation output
- `debug_extraction()`: Troubleshooting function for question extraction

## üìä Bloom's Taxonomy Levels

| Level | Description | Verbs | Examples |
|-------|-------------|-------|----------|
| **K1 (Remember)** | Retrieving knowledge | name, list, recall, identify | "What is X?" |
| **K2 (Understand)** | Determining meaning | explain, describe, interpret | "Explain how Y works" |
| **K3 (Apply)** | Using in new situations | use, apply, demonstrate | "Apply concept Z to..." |
| **K4 (Analyze)** | Examining relationships | analyze, compare, examine | "Compare X and Y" |
| **K5 (Evaluate)** | Making judgments | evaluate, critique, assess | "Evaluate the effectiveness..." |
| **K6 (Create)** | Forming something new | create, design, formulate | "Design a solution for..." |

## üîß Configuration

### Model Configuration
```python
# Modify temperature and other parameters in llm() function
llm = ChatGroq(
    model_name=Model_name,
    temperature=0.4,  # Adjust creativity level
    api_key="your-api-key-here"
)
```

### Text Splitting Configuration
```python
# Adjust chunk size and overlap in rag() function
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,    # Modify chunk size
    chunk_overlap=500   # Modify overlap
)
```

## üìù Sample Output

```
==========================================
QUESTIONS GENERATED BY THE MODEL DEEPSEEK....

**Question 1:** How does the attention mechanism in Transformers compare to traditional sequence modeling approaches in terms of computational efficiency and parallelization capabilities?

**Question 2:** What are the potential limitations of relying solely on attention mechanisms without recurrent or convolutional layers, and how might these limitations affect model performance?

**Question 3:** Why might the multi-head attention approach be more effective than single attention mechanisms for capturing different types of relationships in sequence data?

==========================================
||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||

 EVALUATION DONE BY THE MODEL DEEPSEEK....

question: How does the attention mechanism in Transformers compare to traditional sequence modeling approaches in terms of computational efficiency and parallelization capabilities?
knowledge level: K4
verb found: compare
```

## üö® Troubleshooting

### Common Issues

1. **API Key Errors**: Ensure your Groq API key is valid and has sufficient credits
2. **Ollama Connection**: Verify Ollama is running and the embedding model is available
3. **File Path Issues**: Check that the PDF path exists and is accessible
4. **Question Extraction**: Use `debug_extraction()` function to troubleshoot parsing issues

### Debug Mode
Uncomment the debug lines in the main function to see detailed extraction information:
```python
# questions_llm1 = debug_extraction(response_llm1, "DeepSeek")
```

## üîÑ Workflow

1. **Document Loading**: PDF ‚Üí Text chunks ‚Üí Vector embeddings
2. **Question Generation**: Topic + Context ‚Üí 3 LLMs ‚Üí 3 sets of questions
3. **Cross-Evaluation**: Each LLM evaluates all question sets
4. **Output**: Formatted questions and Bloom's Taxonomy classifications

## üéØ Use Cases

- **Educational Assessment**: Generate questions for testing critical thinking
- **Content Analysis**: Analyze documents for key concepts and relationships
- **Question Quality Evaluation**: Assess cognitive complexity of existing questions
- **Curriculum Development**: Create questions aligned with learning objectives

## üìà Future Enhancements

- Support for multiple document formats (DOCX, TXT, etc.)
- Web interface for easier interaction
- Question difficulty scoring
- Export functionality (JSON, CSV)
- Integration with learning management systems
- Custom evaluation criteria beyond Bloom's Taxonomy

## ü§ù Contributing

This system can be extended by:
- Adding new LLM providers
- Implementing additional evaluation frameworks
- Enhancing question extraction patterns
- Adding support for different document types


## ‚ö†Ô∏è Notes

- Ensure sufficient API credits for multiple model calls
- Large documents may require longer processing times
- Question quality depends on source document content quality
- Evaluation accuracy varies by model capabilities
